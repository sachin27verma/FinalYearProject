{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch numpy pandas sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install tqdm\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  status                                               text\n",
      "0   REAL  Payal has accused filmmaker Anurag Kashyap of ...\n",
      "1   FAKE  A four-minute-long video of a woman criticisin...\n",
      "2   FAKE  Republic Poll, a fake Twitter account imitatin...\n",
      "3   REAL  Delhi teen finds place on UN green list, turns...\n",
      "4   REAL  Delhi: A high-level meeting underway at reside...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Show first few rows to check the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding (Fake -> 0, Real -> 1)\n",
    "label_encoder = LabelEncoder()\n",
    "df['status'] = label_encoder.fit_transform(df['status'])\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'], df['status'], test_size=0.2)\n",
    "\n",
    "# Tokenization and padding (for simplicity, we use a basic tokenizer)\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "def basic_tokenizer(texts):\n",
    "    return [text.split() for text in texts]\n",
    "\n",
    "# Convert text into integer tokens (here we map words to indices)\n",
    "word2idx = {}\n",
    "for text in train_texts:\n",
    "    for word in text.split():\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "def encode_texts(texts):\n",
    "    return [[word2idx[word] for word in text.split() if word in word2idx] for text in texts]\n",
    "\n",
    "train_encodings = encode_texts(train_texts)\n",
    "val_encodings = encode_texts(val_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_sequences(sequences, max_length=MAX_LENGTH):\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_length:\n",
    "            # Pad with 0\n",
    "            padded.append(seq + [0] * (max_length - len(seq)))\n",
    "        else:\n",
    "            # Truncate if longer than max_length\n",
    "            padded.append(seq[:max_length])\n",
    "    return padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = pad_sequences(train_encodings, max_length=MAX_LENGTH)\n",
    "val_encodings = pad_sequences(val_encodings, max_length=MAX_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_size = embed_size\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_size, \"Embedding size must be divisible by number of heads\"\n",
    "        \n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        \n",
    "        # Split the embedding into self.num_heads different pieces\n",
    "        Q = self.query(query).view(N, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(keys).view(N, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(values).view(N, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K])  # [N, heads, query_len, key_len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        \n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=-1)\n",
    "        \n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, V]).reshape(N, -1, self.num_heads * self.head_dim)\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * -(np.log(10000.0) / embed_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, feed_forward_size, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, feed_forward_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feed_forward_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention_out = self.attention(value, key, query, mask)\n",
    "        x = self.norm1(attention_out + query)\n",
    "        forward_out = self.feed_forward(x)\n",
    "        out = self.norm2(forward_out + x)\n",
    "        return self.dropout(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, feed_forward_size, num_classes, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(embed_size)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, num_heads, feed_forward_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(embed_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, x, x, mask)\n",
    "        \n",
    "        out = self.fc_out(x.mean(dim=1))  # Pooling over sequence length\n",
    "        return self.softmax(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [01:22<00:00,  1.13it/s]\n",
      "100%|██████████| 24/24 [00:06<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.6168, Val Loss: 0.5277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [01:21<00:00,  1.15it/s]\n",
      "100%|██████████| 24/24 [00:09<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Train Loss: 0.3578, Val Loss: 0.3453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [01:37<00:00,  1.05s/it]\n",
      "100%|██████████| 24/24 [00:06<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Train Loss: 0.3409, Val Loss: 0.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [01:13<00:00,  1.26it/s]\n",
      "100%|██████████| 24/24 [00:06<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Train Loss: 0.3367, Val Loss: 0.3376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [01:17<00:00,  1.20it/s]\n",
      "100%|██████████| 24/24 [00:07<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Train Loss: 0.3310, Val Loss: 0.3306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [01:24<00:00,  1.10it/s]\n",
      "100%|██████████| 24/24 [00:08<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Train Loss: 0.3275, Val Loss: 0.3394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [01:32<00:00,  1.00it/s]\n",
      "100%|██████████| 24/24 [00:06<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Train Loss: 0.3251, Val Loss: 0.3279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [01:13<00:00,  1.27it/s]\n",
      "100%|██████████| 24/24 [00:06<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Train Loss: 0.3212, Val Loss: 0.3238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [01:14<00:00,  1.24it/s]\n",
      "100%|██████████| 24/24 [00:06<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Train Loss: 0.3191, Val Loss: 0.3301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [01:12<00:00,  1.28it/s]\n",
      "100%|██████████| 24/24 [00:06<00:00,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Train Loss: 0.3190, Val Loss: 0.3247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "EMBED_SIZE = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 3\n",
    "FEED_FORWARD_SIZE = 512\n",
    "NUM_CLASSES = 2\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = FakeNewsDataset(train_encodings, train_labels)\n",
    "val_dataset = FakeNewsDataset(val_encodings, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = TransformerModel(VOCAB_SIZE, EMBED_SIZE, NUM_HEADS, NUM_LAYERS, FEED_FORWARD_SIZE, NUM_CLASSES, DROPOUT)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    train_loss = np.mean(train_losses)\n",
    "    \n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            input_ids = batch['input_ids']\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            output = model(input_ids)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            val_losses.append(loss.item())\n",
    "    \n",
    "    val_loss = np.mean(val_losses)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "lua"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: In context of the report on missing girls, the tweet claimed that the state topped in crime against women. FactChecker looked at data from the NCRB’s Crime in India 2021 report and found that Rajasthan, which has an Indian National Congress (INC) government, ranked second on absolute number of crimes against women, and sixth on crime rate.\n",
      "Prediction: Real\n",
      "\n",
      "Text: fake hai bhai 90uokjdflkd09lkfj fake hai bhai fake hai bhai\n",
      "Prediction: Real\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sksac\\AppData\\Local\\Temp\\ipykernel_7520\\4157026811.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('transformer_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#save and test\n",
    "torch.save(model.state_dict(), 'transformer_model.pth')\n",
    "\n",
    "model = TransformerModel(VOCAB_SIZE, EMBED_SIZE, NUM_HEADS, NUM_LAYERS, FEED_FORWARD_SIZE, NUM_CLASSES, DROPOUT)\n",
    "model.load_state_dict(torch.load('transformer_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: - Prediction: Real\n",
      "Text: - Prediction: Real\n",
      "Text: - Prediction: Real\n",
      "Text: - Prediction: Real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sksac\\AppData\\Local\\Temp\\ipykernel_7520\\3871392855.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('transformer_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "#save and test\n",
    "torch.save(model.state_dict(), 'transformer_model.pth')\n",
    "\n",
    "model = TransformerModel(VOCAB_SIZE, EMBED_SIZE, NUM_HEADS, NUM_LAYERS, FEED_FORWARD_SIZE, NUM_CLASSES, DROPOUT)\n",
    "model.load_state_dict(torch.load('transformer_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_texts = [\n",
    "    \"The earth is flat\",\n",
    "    \"The earth is round\",\n",
    "    \"The moon is made of cheese\",\n",
    "    \"NEW DELHI: Companies having not less than 300 workers will soon be allowed to hire and fire workers without seeking prior government permission, with the labour ministry proposing changes to rules in a bill introduced in Lok Sabha on Saturday.The proposal, which was the bone of contention between the ministry and trade unions, is part of the Industrial Relation Code Bill 2020.Currently, only those industrial establishments with less than 100 employees are permitted to hire and fire their staff without permission of the government.The bill was introduced by labour minister Santosh Gangwar amid opposition from Congress and few other parties.The Industrial Relation Code Bill 2019 was introduced in Lok Sabha last year and subsequently sent to the Parliamentary Standing Committee on Labour. This bill was withdrawn on Saturday.An earlier draft bill circulated by the labour ministry for discussion had also proposed the criteria that companies having not less than 300 employees can hire and fire without the government's permission. However, this provision faced stiff opposition from trade unions and was not included in the 2019 bill.Earlier this year, the Parliamentary committee also made a case of allowing companies having less than 300 workers to go for retrenchment of staff or closure without government permission.States like Rajasthan have already increased the threshold to 300 workers, which according to the labour ministry has resulted in an increase in employment and a decrease in retrenchment, the committee had pointed out in its report.With regard to the threshold, the government has proposed Section 77(1) in the The Industrial Relation Code 2020.According to the Section, the provisions of this Chapter (lay-off, retrenchment and closure in certain establishment) shall apply to an industrial establishment (not being an establishment of a seasonal character or in which work is performed only intermittently) in which not less than three hundred workers, or such higher number of workers as may be notified by the appropriate Government, were employed on an average per working day in the preceding twelve months.Apart from this code, two others- Occupational Safety, Health And Working Conditions Code, 2020 and the Code On Social Security, 2020- were also introduced by the minister in Lok Sabha.Among others, Congress leaders- Manish Tewari and Shashi Tharoor- opposed the introduction of the three bills.Tewari noted these three bills are fundamentally changed versions of their earlier forms and urged the minister to withdraw them and hold wider consultations before introducing them.These bills are also a blow to the rights of workers, he added.With respect to the industrial relations code, Tharoor said it severely restricts the right of workers to strike and also allows state or central governments to amend the threshold for applicability relating to layoffs and retrenchment.In Lok Sabha, Gangwar said that over 29 labour laws have been merged into four codes and that one of them has already been passed.The Code on Wages Bill, 2019 was passed by Parliament last year.Gangwar noted that the government engaged in wider consultations over these bills with various stakeholders and that more than 6,000 comments were received online on the bills.These bills were later sent to a standing committee and 174 of its 233 recommendations have been accepted, the minister said.\"\n",
    "]\n",
    "\n",
    "test_encodings = encode_texts(test_texts)\n",
    "test_encodings = pad_sequences(test_encodings, max_length=MAX_LENGTH)\n",
    "\n",
    "test_dataset = FakeNewsDataset(test_encodings, pd.Series([0] * len(test_texts)))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids']\n",
    "        output = model(input_ids)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        predictions.extend(pred.tolist())\n",
    "\n",
    "for text, pred in zip(test_texts, predictions):\n",
    "    print(f\"Text: - Prediction: {'Fake' if pred == 0 else 'Real'}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m, in \u001b[0;36mFakeNewsDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m----> 8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencodings\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong),\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39miloc[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     10\u001b[0m     }\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9879\n"
     ]
    }
   ],
   "source": [
    "# cheak the accuracy on test data and also provide some provide insightful results\n",
    "test_dataset = FakeNewsDataset(val_encodings, val_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids']\n",
    "        output = model(input_ids)\n",
    "        predictions.append(output.argmax(dim=1).item())\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for prediction, label in zip(predictions, val_labels):\n",
    "    if prediction == label:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       377\n",
      "           1       0.99      0.99      0.99       367\n",
      "\n",
      "    accuracy                           0.99       744\n",
      "   macro avg       0.99      0.99      0.99       744\n",
      "weighted avg       0.99      0.99      0.99       744\n",
      "\n",
      "[[372   5]\n",
      " [  4 363]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#provide precision, recall, f1-score and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(val_labels, predictions))\n",
    "print(confusion_matrix(val_labels, predictions))\n",
    "\n",
    "\n",
    "#provide some insights\n",
    "\n",
    "#image of confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(val_labels, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide some insightful results\n",
    "\n",
    "# Get attention weights\n",
    "\n",
    "def get_attention_weights(model, text):\n",
    "    model.eval()\n",
    "    encodings = encode_texts([text])\n",
    "    encodings = pad_sequences(encodings, max_length=MAX_LENGTH)\n",
    "    input_ids = torch.tensor(encodings, dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "    \n",
    "    attention_weights = []\n",
    "    for block in model.transformer_blocks:\n",
    "        for i, layer in enumerate(block.attention.children()):\n",
    "            if i % 4 == 0:\n",
    "                attention_weights.append(layer(output, output, output, mask=None))\n",
    "    \n",
    "    return attention_weights\n",
    "\n",
    "text = \"This is a real news article\"\n",
    "attention_weights = get_attention_weights(model, text)\n",
    "\n",
    "# Plot attention weights\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_attention_weights(attention_weights):\n",
    "    fig, axs = plt.subplots(1, len(attention_weights), figsize=(20, 5))\n",
    "    for i, attention in enumerate(attention_weights):\n",
    "        axs[i].imshow(attention.squeeze().numpy())\n",
    "        axs[i].set_title(f\"Layer {i+1}\")\n",
    "    plt.show()\n",
    "\n",
    "plot_attention_weights(attention_weights)\n",
    "\n",
    "# Get attention weights for fake news\n",
    "text = \"This is a fake news article\"\n",
    "\n",
    "attention_weights = get_attention_weights(model, text)\n",
    "plot_attention_weights(attention_weights)\n",
    "\n",
    "# Get attention weights for a longer text\n",
    "text = \"This is a real news article. \" * 10\n",
    "\n",
    "attention_weights = get_attention_weights(model, text)\n",
    "\n",
    "plot_attention_weights(attention_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
